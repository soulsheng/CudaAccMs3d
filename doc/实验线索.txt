1、绪论
问题：骨骼动画帧速低。
诊断：计算时间长，渲染时间长。
路线：缩短时间，包括计算和渲染两个环节的时间。
方法：多核并行运算，先用CPU多核，再用GPU多核，择优采纳。
预测：CPU多核并行和GPU多核并行两者都能够缩短计算时间。GPU多核同时能够缩短渲染时间，预计GPU多核比CPU多核将更大幅度地缩短时间。

2、展开
    骨骼动画数据结构：三角面对顶点的索引、顶点、顶点对骨骼矩阵的索引、骨骼矩阵、骨骼动作关键帧。
    计算涉及的算法：骨骼动作关键帧修改骨骼矩阵，骨骼矩阵修改顶点的位置坐标。
    渲染涉及的算法：顶点数据从内存传输到图像处理器GPU的显存，GPU进行渲染。渲染方式分为VBO渲染和普通渲染这两种，对应两种不同的数据传输方式，即逐帧重复传输和一次性传输。
    计算与渲染的相关性：如果修改顶点的计算所用的处理器是CPU，渲染方式是普通渲染，顶点数据传输方式是逐帧重复传输；如果计算所用的处理器是GPU，渲染方式是VBO渲染，顶点数据传输方式是一次性传输。
    时间与问题规模的相关性：问题规模体现在数据量，当数据量成倍增长时，耗时将以什么速度增长。耗时增长速度，最终影响优化算法的适用范围和实际效果。
    时间与硬件配置的相关性：CPU和GPU性能越高，相应的并行加速性能越好。本文采用当前较为高端的CPU和GPU，CPU的型号是Intel i7 3770，4核8线程，主频3.5G，搭配内存带宽为100G容量为8G；GPU的型号是Nvidia Geforce GTX670，1344核，shader频率0.9G，搭配显存带宽为200G容量为2G。

3、基础实验
3.1 对CPU单线程算法执行耗时分析 明确性能瓶颈和优化目标
    细分环节包括3个，骨骼动作关键帧修改骨骼矩阵、骨骼矩阵修改顶点的位置坐标、顶点数据从内存传输到显存并且渲染，分别简称为，修改骨骼、修改顶点、渲染。
    【实验3.1】不同规模的数据，分别记录CPU单线程的总耗时以及各个细分环节的耗时。
    根据实验分析结果，找到耗时比例较大的环节，作为优化目标，准备执行多核并行运算。修改顶点这一环节耗时最多，比例多达X%，将它作为优化目标。修改骨骼这一环节的耗时比例小至Y%，而且骨骼之间存在依赖关系，不适于并行运算。渲染环节能否优化以及如何优化，跟上一环节即修改顶点相关，修改顶点若采用CPU算法，渲染无法优化；若采用GPU算法，渲染采用VBO进行优化。

3.2 为适应多核并行，做一定的预备工作
    预备工作包括，使单核单线程算法的性能达到最优、降低算法结构的复杂性。
    单线程算法流程细分为以下几个环节，读取上一帧的骨骼矩阵、读取上一帧的顶点坐标、通过上一帧的骨骼矩阵的逆矩阵修改上一帧的顶点坐标获取顶点坐标初始值、读取当前帧的骨骼矩阵、通过当前帧的骨骼矩阵修改顶点坐标初始值获取新的顶点坐标、写入新的顶点坐标。涉及3个读数据操作、2个坐标与矩阵之间的运算操作、1个写数据操作。采用以空间换时间的方法，额外备份一份顶点坐标初始值，将节省1个读操作和1个运算操作。
    【实验3.2.1】不同规模的数据，分别记录减少运算量之后的总耗时以及各个细分环节的耗时。
    根据实验分析结果，时间缩短为原来的X%。
    算法结构方面，原始算法的所有面片连带顶点分按照网格进行分组，导致顶点被切分。重新将所有顶点合并在一起，将简化算法结构的复杂性。
    【实验3.2.2】不同规模的数据，分别记录顶点合并之后的总耗时以及各个细分环节的耗时。
    根据实验分析结果，这个优化对于单线程，时间没有缩短，但有利于并行运算，在并行运算时将对顶点合并前后的时间进行对比分析。至此，单线程算法接近最优，以此作为后续多核并行的基础算法和性能参考基准。

3.3 基于OpenMP的CPU多核并行
    开启OpenMP的方法是，编译选项新增"/openmp"，在需要并行的循环体之前添加指令"#pragma parallel for"。
   【实验3.3】不同规模的数据，CPU多核并行的总耗时以及各个细分环节的耗时。
    根据实验分析结果，时间缩短为原来单线程的X%。

3.4 基于CUDA的GPU多核并行
    算法移植到CUDA，初步配备相应的资源，包括线程数目、Gloabal/Shared/Constant/Register等各种存储器。线程数目，初步设为每个BLOCK块256线程，32个块；Global全局显存用于将顶点VBO转化为CUDA可操作资源以及存储骨骼关节的矩阵，Shared共享存储器和Constant常量存储器暂时没有用到；寄存器跟kernel核函数定义的变量个数有关，但不完全相等，具体使用量的查看和优化在后面的章节进行。
    CUDA算法细分为，CPU修改关节矩阵、关节矩阵从内存传输到显存、顶点VBO执行Map转为可以被CUDA操作的显存、在GPU执行kernel修改顶点坐标、顶点坐标显存UnMap为VBO、渲染VBO。
   【实验3.4】不同规模的数据，初步配置CUDA资源的总耗时以及各个细分环节的耗时。
    根据实验分析结果，总时间缩短为原来的X%。由于采用GPU计算顶点，顶点数据存储在VBO，渲染时用VBO加速，渲染时间缩短为原来单线程的Y%。
    初步配置CUDA资源，只发挥出部分的GPU性能。更深层次地寻找性能瓶颈，确立优化目标。根据实验分析结果，当模型复杂度小于X时，VBO和CUDA显存之间的Map占用时间高达Y%，没有优化余地；当模型复杂度小于X时，kernel修改顶点占用时间的比例高达X%，所以优化目标是kernel，以kernel使用的时钟周期作为衡量指标，并以这个版本作为参考基准。

4、优化基于CUDA的GPU多核并行
4.1 优化CUDA线程数目
    通过CUDA SDK提供的工具Visual Profiler分析得到每个线程用到的寄存器个数regperthread，在CUDA SDK提供的表格工具OccupationCalculator.xls里面，输入每个线程用到的寄存器个数和每个块用到的共享存储器个数，查看单个块的线程数目和同时处于激活状态的warp束数的相关图表，一般情况最优的单个块的线程数目会有多个选项供选择。单个块的线程数目确定以后，处于激活状态的块的数目可以得到，最终总块数设置为：激活块的数目的两倍再乘于MP多重处理器单元的个数。
   【实验4.1】不同规模的数据，优化CUDA线程数目的总耗时以及各个细分环节的耗时。
    根据实验分析结果，kernel时钟周期降为原来的X%。

4.2 优化存储器的类型
    kernel核函数参与运算的数据有顶点坐标和关节矩阵，它们都存储在Global全局存储器。Global存储器的读写耗费很长的时间，多达几百个GPU时钟周期，改用其他存储方式可以获得更优的性能。顶点的数据量很大而且不存在线程之间共享，所以没有优化存储的潜力。关节矩阵被所有块的所有线程共享，而且数据量很小，适合存储在Constant常量存储器。
   【实验4.2】不同规模的数据，优化存储器的总耗时以及各个细分环节的耗时。
    根据实验分析结果，kernel时钟周期降为原来的X%。

4.3 优化存储器的访问
    CUDA执行kernel时以warp束或半束为单位分组并行，warp束内的32个线程或半束的16个线程，如果访问地址相邻的存储器，将合并读写，从而减少读写次数和寻址时间，最终提高读写效率。原先算法按线程ID先索引面片，然后通过面片间接索引连续的3个顶点，每个线程访问地址相邻的三个顶点，造成相邻线程并行访问的存储地址反而不连续，不能合并访问。改进过的算法，去除面片这个层次的索引，直接按线程ID索引顶点，从而相邻线程可以访问相邻顶点的存储地址，成功构造合并访问的条件。
   【实验4.3】不同规模的数据，优化存储器的总耗时以及各个细分环节的耗时。
    根据实验分析结果，kernel时钟周期降为原来的X%。    

5、结论
    GPU多核的帧速是CPU多核的帧速的N倍，采纳基于CUDA的GPU多核并行能够最大化缩短时间提升帧速。基于CUDA的GPU多核并行，采用通用的资源配置方案，计算和渲染总时间缩短为原来的T%，帧速增幅为F%；优化CUDA的资源配置以后，帧速增幅为F%。
    CUDA还有另外一种用于优化性能的资源，即高速的Shared存储器，在本算法中被闲置。而备份的顶点初始坐标，存储在低速的Global存储器，如果能够将这部分数据移到Shared存储器，时间效率还有较大的提升空间。

